{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "od-Vfj39KUTw"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yfjkkHbsbMUn"
   },
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = ''.join(re.findall(r'[ а-я]', text))\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "colab_type": "code",
    "id": "f-MI5NfmOlxV",
    "outputId": "f992cec1-b399-4f31-fd6c-d3c04e7517f7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 1/22 [00:00<00:02,  7.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206076403\n",
      "206062060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  9%|▉         | 2/22 [00:01<00:08,  2.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206073010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 4/22 [00:01<00:05,  3.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206076829\n",
      "206076655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 23%|██▎       | 5/22 [00:01<00:04,  3.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206050137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 27%|██▋       | 6/22 [00:02<00:07,  2.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206064673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 32%|███▏      | 7/22 [00:03<00:06,  2.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206076211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 36%|███▋      | 8/22 [00:08<00:26,  1.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206071706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 10/22 [00:09<00:13,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206075749\n",
      "206050630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 12/22 [00:10<00:08,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206076298\n",
      "206068597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 59%|█████▉    | 13/22 [00:10<00:05,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206069753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 64%|██████▎   | 14/22 [00:15<00:13,  1.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206065715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 68%|██████▊   | 15/22 [00:15<00:09,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206071176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 73%|███████▎  | 16/22 [00:15<00:06,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206062608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 18/22 [00:17<00:03,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206076478\n",
      "206062712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 86%|████████▋ | 19/22 [00:18<00:02,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206071000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 91%|█████████ | 20/22 [00:18<00:01,  1.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206063953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [00:18<00:00,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206073682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_page(url):\n",
    "    page = urllib.request.urlopen(url).read().decode('utf-8')\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "def parse_2ch():\n",
    "    posts = get_page('https://2ch.hk/b/').select('.post_type_oppost a')\n",
    "    \n",
    "    links = set()\n",
    "    for post in posts:\n",
    "        idx = post.get('id')\n",
    "        if isinstance(idx, str):\n",
    "            links.add(idx)\n",
    "            \n",
    "    messages = []\n",
    "    for idx in tqdm(links):\n",
    "        print(idx)\n",
    "        raw_messages = get_page(f'https://2ch.hk/b/res/{idx}.html').select('.post__message')\n",
    "        for message in raw_messages:\n",
    "            message = preprocess(message.text)\n",
    "            if len(message) > 10:\n",
    "                messages.append(message)\n",
    "    \n",
    "    return messages\n",
    "\n",
    "dwach = parse_2ch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 574
    },
    "colab_type": "code",
    "id": "TzxHnhQVN7BX",
    "outputId": "e476a0b7-59dc-4380-dc4b-b8493cb90f23"
   },
   "outputs": [],
   "source": [
    "bibl = []\n",
    "\n",
    "with open('bibl.txt', 'r', encoding=\"windows-1251\") as file:\n",
    "    for line in file:\n",
    "        line = preprocess(line)\n",
    "        if len(line) > 10:\n",
    "            bibl.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "colab_type": "code",
    "id": "zKdHauyIKguJ",
    "outputId": "31c517d1-e2fe-4adb-c28c-22e8bfe43786"
   },
   "outputs": [],
   "source": [
    "tolstoy = []\n",
    "\n",
    "with open('tolst.txt', 'r', encoding=\"windows-1251\") as file:\n",
    "    for line in file:\n",
    "        line = preprocess(line)\n",
    "        if len(line) > 10:\n",
    "            tolstoy.append(line)\n",
    "tolstoy=tolstoy[:4000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "891GJWsCKUT4"
   },
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, corpora, tokenizer, max_len=20, vocab_size=5000):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        counts = {}\n",
    "        \n",
    "        for sentence in corpora:\n",
    "            for token in sentence:\n",
    "                counts[token] = counts.get(token, 0) + 1\n",
    "        \n",
    "        l = sorted(counts.items(), key=lambda x: -x[1])[:vocab_size-2]\n",
    "        \n",
    "        print('least used token:', l[-1])\n",
    "        print('vocab size:', len(l), '(+2)')\n",
    "        \n",
    "        self.t2i = {\"<pad>\" : 0, \"<unk>\" : 1}\n",
    "        self.i2t = {0 : \"<pad>\", 1 : \"<unk>\"}\n",
    "              \n",
    "        for token, _ in l:\n",
    "            self.i2t[len(self.i2t)] = token\n",
    "            self.t2i[token] = len(self.t2i)\n",
    "    \n",
    "    def tokenize(self, sentence, pad=True):\n",
    "        if pad:\n",
    "            sentence = sentence[:self.max_len]\n",
    "            while len(sentence) < self.max_len:\n",
    "                sentence.append('<pad>')\n",
    "        \n",
    "        indices = []\n",
    "        for token in sentence:\n",
    "            if token in self.t2i:\n",
    "                indices.append(self.t2i[token])\n",
    "            else:\n",
    "                indices.append(1)\n",
    "        \n",
    "        return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "Wl3zeFSUKUT8",
    "outputId": "322cea2d-59e1-4326-a0aa-4ad2e8f3dbab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4392\n",
      "4000\n",
      "30733\n",
      "least used token: ('худого', 19)\n",
      "vocab size: 4998 (+2)\n"
     ]
    }
   ],
   "source": [
    "# me: mom can we have oversampling?\n",
    "# mom: but we have oversampling at home\n",
    "# oversampling at home:\n",
    "dataset_a = dwach+dwach+dwach+dwach\n",
    "dataset_b = tolstoy\n",
    "dataset_c = bibl\n",
    "\n",
    "print(len(dataset_a))\n",
    "print(len(dataset_b))\n",
    "print(len(dataset_c))\n",
    "\n",
    "vocab = Vocab(dataset_a + dataset_b + dataset_c, preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0OKvCmcGKUT_"
   },
   "outputs": [],
   "source": [
    "class StyleDataset(Dataset):\n",
    "    def __init__(self, corpora, vocab):\n",
    "        self.vocab = vocab\n",
    "        self.samples = []\n",
    "        self.labels = []\n",
    "        \n",
    "        for i, dataset in enumerate(corpora):\n",
    "            self.samples += dataset\n",
    "            self.labels += [i] * len(dataset)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        X = self.vocab.tokenize(self.samples[idx])\n",
    "        y = [self.labels[idx]] * len(X)\n",
    "        return torch.LongTensor(X), torch.LongTensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "aaXow-McKUUC",
    "outputId": "d1f0b074-1aa8-489a-a41e-fde5152c7277"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([  13,  400, 1929,    1, 1540, 1688,   10,  829,   13, 1431, 1861,  862,\n",
       "         3725,   15,   13, 1540,    0,    0,    0,    0]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = StyleDataset([dataset_a, dataset_b, dataset_c], vocab)\n",
    "\n",
    "# todo: this should be done before oversampling\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [len(dataset)-512, 512])\n",
    "\n",
    "train = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test = DataLoader(test_dataset, batch_size=64)\n",
    "\n",
    "dataset[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o8tVAVfFKUUJ"
   },
   "outputs": [],
   "source": [
    "def fetch_embeddings(embeddings, vocab, embedding_dim=300):\n",
    "    weights = torch.randn(len(vocab.t2i), embedding_dim) / 10\n",
    "    \n",
    "    print('reading file', embeddings)\n",
    "    print('this may take a while...')\n",
    "    \n",
    "    with open(embeddings) as file:\n",
    "        for line in file:\n",
    "            data = line.split()\n",
    "            if len(data) == embedding_dim + 1:\n",
    "                token = data[0]\n",
    "                vector = torch.Tensor([float(x) for x in data[1:]])\n",
    "                if token in vocab.t2i:\n",
    "                    weights[vocab.t2i[token]] = vector\n",
    "    \n",
    "    return weights\n",
    "\n",
    "\n",
    "class StyleClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embeddings,\n",
    "        embedding_dim=300,\n",
    "        num_classes=3,\n",
    "        hidden_dim=50,\n",
    "        num_layers=1,\n",
    "        rnn_dropout=0,\n",
    "        bidirectional=False):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embed = nn.Embedding.from_pretrained(embeddings)\n",
    "        \n",
    "        self.rnn = nn.GRU(\n",
    "            embedding_dim,\n",
    "            hidden_dim, \n",
    "            num_layers=num_layers, \n",
    "            bidirectional=bidirectional, \n",
    "            dropout=rnn_dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.head = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(hidden_dim, num_classes),\n",
    "            nn.LogSoftmax(dim=2)\n",
    "        )\n",
    "        \n",
    "    def forward(self, X):\n",
    "        X = self.embed(X)\n",
    "        X, _ = self.rnn(X)\n",
    "        X = self.head(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HE6aZSjUuPwh"
   },
   "outputs": [],
   "source": [
    "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.ru.vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lNmAuJHftc58"
   },
   "outputs": [],
   "source": [
    "!touch empty.vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "8_AbBsD7KUUM",
    "outputId": "321a2333-1731-473f-864b-7a20612fb7c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading file wiki.ru.vec\n",
      "this may take a while...\n"
     ]
    }
   ],
   "source": [
    "embeddings = fetch_embeddings('wiki.ru.vec', vocab, embedding_dim=300)\n",
    "model = StyleClassifier(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G-m08RFtKUUQ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.3150, 0.3388, 0.3462],\n",
       "         [0.2650, 0.3187, 0.4163],\n",
       "         [0.2723, 0.3418, 0.3859],\n",
       "         [0.2940, 0.3608, 0.3452],\n",
       "         [0.2852, 0.3137, 0.4011],\n",
       "         [0.3079, 0.3679, 0.3241],\n",
       "         [0.2975, 0.2985, 0.4040],\n",
       "         [0.2827, 0.3084, 0.4089],\n",
       "         [0.3227, 0.3279, 0.3494],\n",
       "         [0.2923, 0.3249, 0.3828],\n",
       "         [0.2706, 0.2990, 0.4303],\n",
       "         [0.3042, 0.3463, 0.3494],\n",
       "         [0.2902, 0.3347, 0.3751],\n",
       "         [0.3122, 0.2991, 0.3887],\n",
       "         [0.3190, 0.3401, 0.3408],\n",
       "         [0.2953, 0.3212, 0.3835],\n",
       "         [0.2925, 0.3257, 0.3818],\n",
       "         [0.2706, 0.3565, 0.3729],\n",
       "         [0.2758, 0.3369, 0.3873],\n",
       "         [0.2738, 0.3294, 0.3968]]], grad_fn=<ExpBackward>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_batch = dataset[0][0].view(1, -1)\n",
    "model(test_batch).exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ElGnnan7KUUU"
   },
   "outputs": [],
   "source": [
    "lr = 5e-2\n",
    "num_epochs = 10\n",
    "device = torch.device('cpu')\n",
    "\n",
    "#model = StyleClassifier(len(vocab.i2t), 'fasttext.vec')\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "colab_type": "code",
    "id": "u2V5pB1MKUUc",
    "outputId": "7c78e1de-19c7-488e-92ac-51bbfae130cd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 604/604 [00:14<00:00, 42.49it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 102.40it/s]\n",
      "  1%|          | 5/604 [00:00<00:14, 41.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 3.1913817822933197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 604/604 [00:14<00:00, 42.27it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 103.31it/s]\n",
      "  1%|          | 5/604 [00:00<00:12, 48.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 3.2933852672576904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 604/604 [00:12<00:00, 49.20it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 115.95it/s]\n",
      "  1%|          | 5/604 [00:00<00:12, 47.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 3.126163125038147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 604/604 [00:12<00:00, 48.49it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 117.16it/s]\n",
      "  1%|          | 5/604 [00:00<00:12, 47.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 3.1038631349802017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 604/604 [00:14<00:00, 41.22it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 86.21it/s]\n",
      "  1%|          | 4/604 [00:00<00:16, 36.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 3.0791245698928833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 604/604 [00:13<00:00, 46.44it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 113.21it/s]\n",
      "  1%|          | 5/604 [00:00<00:12, 46.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 3.259096711874008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 604/604 [00:15<00:00, 40.09it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 86.79it/s]\n",
      "  1%|          | 4/604 [00:00<00:16, 35.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 3.1410126984119415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 468/604 [00:14<00:03, 35.55it/s]"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    for X, y in tqdm(train):\n",
    "        X.to(device)\n",
    "        y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        preds = model(X)\n",
    "        \n",
    "        loss = criterion(preds.view(-1, 3), y.view(-1))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_losses.append(loss.item())\n",
    "    \n",
    "    test_loss = 0\n",
    "    for X, y in tqdm(test):\n",
    "        X.to(device)\n",
    "        y.to(device)\n",
    "        \n",
    "        preds = model(X)\n",
    "        loss = criterion(preds.view(-1, 3), y.view(-1))\n",
    "        \n",
    "        test_loss += loss.item()\n",
    "    \n",
    "    test_losses.append(test_loss)\n",
    "    print('test loss:', test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 521
    },
    "colab_type": "code",
    "id": "_wP58MHrKUUf",
    "outputId": "4f02da3a-db7c-4639-f402-aa02d845cfe4"
   },
   "outputs": [],
   "source": [
    "plt.plot(train_losses)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(test_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oz0rhlFFKUUo"
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "def print_colored(sequence, intensities, delimeter=' '):\n",
    "    html = delimeter.join([\n",
    "        # https://en.wikipedia.org/wiki/Subtractive_color\n",
    "        f'<span style=\"background: rgb({255*(1-x[1]-x[2])}, {255*(1-x[0]-x[2])}, {255*(1-x[0]-x[1])})\">{c}</span>'\n",
    "        for c, x in zip(sequence, intensities) \n",
    "    ])\n",
    "    display(HTML(html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "QQPrQkwVKUUs",
    "outputId": "d641e009-63c1-472e-b331-aa8d09cc4957"
   },
   "outputs": [],
   "source": [
    "samples = [\n",
    "    'Сап б, есть одна тян. Двачую однаапррвавапвпв анон',\n",
    "    'князь болконский с удивлением увидел',\n",
    "    'засмеялся проиграл тред не нашел создал',\n",
    "    'хахаха ну ты даешь',\n",
    "    'Сап двач, есть однаапррвавапвпв хэштег тян, она как будто со вниманием слушала рассказ князя Василья',\n",
    "    ' '.join(dataset_a[111]), # Двач, должен быть красным\n",
    "    ' '.join(dataset_b[111]), # bible, должна быть зелёной\n",
    "    ' '.join(dataset_c[333]), # Tolstoy, должен быть синим\n",
    "    ' '.join(dataset_a[6] + dataset_b[6] + dataset_c[6])\n",
    "]\n",
    "\n",
    "model = model.cpu()\n",
    "\n",
    "t = 1 # температура, для лучшей визуализации\n",
    "\n",
    "for sample in samples:\n",
    "    sentence = preprocess(sample)\n",
    "    X = torch.LongTensor(vocab.tokenize(sample, pad=False)).view(-1, 1)\n",
    "    scores = model(X).view(-1, 3).mul(t).softmax(dim=1)\n",
    "    maxes, _ = torch.max(scores, dim=1)\n",
    "    scores -= ((1-maxes)/2).view(-1, 1) # так будет ровно один \"полный\" цвет\n",
    "    #print(scores)\n",
    "    print_colored(sentence, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N9mHKg8TT6W8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "rnn.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
